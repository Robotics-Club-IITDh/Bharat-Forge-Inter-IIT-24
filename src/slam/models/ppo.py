# -*- coding: utf-8 -*-
"""ppo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HuurXiehZH1Y706j225cioBivcbRuTNj
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from gym import spaces
import gym

class StaticObstacleEnv(gym.Env):
    def __init__(self, lidar_resolution=360, goal_position=np.array([9, 9])):
        super(StaticObstacleEnv, self).__init__()
        self.grid_size = 10
        self.num_obstacles = 4
        self.goal_position = goal_position
        self.lidar_resolution = lidar_resolution

        # Precompute angles for lidar
        self.lidar_angles = np.linspace(0, 2 * np.pi, self.lidar_resolution, endpoint=False)

        # Action Space: [vx, vy]
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )

        # Observation Space: Agent state + obstacles + LiDAR data
        self.observation_space = spaces.Box(
            low=0.0,
            high=self.grid_size,
            shape=(5 + 2 * self.num_obstacles + self.lidar_resolution,),
            dtype=np.float32
        )

        self.reset()

    def reset(self):
        # More strategic initial placement
        self.agent_position = np.array([1.0, 1.0])  # Start close to bottom-left

        # Ensure obstacle strategic placement
        self.obstacle_positions = self._generate_strategic_obstacles()

        self.steps = 0
        self.max_steps = 500  # Increased max steps
        return self._get_observation()

    def _generate_strategic_obstacles(self):
        # Generate obstacles with strategic constraints
        obstacles = []
        attempts = 0
        while len(obstacles) < self.num_obstacles and attempts < 100:
            candidate = np.random.uniform(2, self.grid_size - 2, (1, 2))[0]

            # Check distance from goal and agent
            if (np.linalg.norm(candidate - self.goal_position) > 2 and
                np.linalg.norm(candidate - self.agent_position) > 2 and
                all(np.linalg.norm(candidate - obs) > 1.5 for obs in obstacles)):
                obstacles.append(candidate)

            attempts += 1

        return np.array(obstacles)

    def _compute_reward_and_done(self, dist_to_goal, min_obstacle_dist):
        # More nuanced reward function
        if min_obstacle_dist < 0.5:
            return -20.0, True  # Stronger collision penalty

        if dist_to_goal < 0.5:
            return 1000.0, True  # Significantly increased goal reward

        # Hierarchical reward structure
        proximity_reward = 10 * max(0, 1 - dist_to_goal / self.grid_size)
        direction_reward = 5 * (1 - np.linalg.norm(self.agent_position - self.goal_position) /
                                 np.linalg.norm(np.array([1.0, 1.0]) - self.goal_position))

        # Penalize taking too many steps
        step_penalty = -0.1 * (self.steps / self.max_steps)

        total_reward = proximity_reward + direction_reward + step_penalty
        return total_reward, False

    # Rest of the environment methods remain the same as in the original implementation

class PPOModel(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=512):  # Increased hidden dimension
        super(PPOModel, self).__init__()

        # More complex network architecture
        self.layer_norm_input = nn.LayerNorm(state_dim)

        # Increased network depth and width
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, hidden_dim // 2)

        self.actor_mean = nn.Linear(hidden_dim // 2, action_dim)
        self.actor_std = nn.Linear(hidden_dim // 2, action_dim)
        self.critic = nn.Linear(hidden_dim // 2, 1)

        # Advanced weight initialization
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # He initialization for ReLU
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.01)

    def forward(self, state):
        x = self.layer_norm_input(state)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        return x

    def get_action(self, state):
        x = self.forward(state)
        mean = self.actor_mean(x)
        log_std = torch.clamp(self.actor_std(x), min=-5, max=2)
        std = torch.exp(log_std)

        # Prevent numerical instability
        mean = torch.nan_to_num(mean, nan=0.0)
        std = torch.nan_to_num(std, nan=1.0)

        # Create distribution
        dist = torch.distributions.Normal(mean, std)

        # Reparameterization trick
        action = dist.rsample()
        action = torch.clamp(action, -1.0, 1.0)

        log_prob = dist.log_prob(action).sum(dim=-1)
        entropy = dist.entropy().mean()

        return action, log_prob, entropy

    def get_value(self, state):
        x = self.forward(state)
        return self.critic(x)

def train_ppo(env, model, num_episodes=200, gamma=0.99, lr=0.0002, clip_ratio=0.2):
    # Adaptive learning rate and clip ratio
    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-5, weight_decay=1e-4)

    episode_rewards = []
    success_records = []

    # Learning rate scheduler
    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0
        log_probs, values, rewards, states, actions = [], [], [], [], []
        success = 0

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)

            # Get action with exploration
            action, log_prob, _ = model.get_action(state_tensor)
            value = model.get_value(state_tensor)

            next_state, reward, done, info = env.step(action.detach().numpy()[0])

            states.append(state_tensor)
            actions.append(action)
            log_probs.append(log_prob)
            values.append(value)
            rewards.append(reward)
            total_reward += reward
            state = next_state

            if done and info.get('success', False):
                success = 1

        success_records.append(success)
        episode_rewards.append(total_reward)

        # Advanced Generalized Advantage Estimation
        values = torch.cat(values)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        returns, advantages = compute_gae(rewards, values, gamma)

        # Multiple optimization steps
        for _ in range(5):
            new_values = model.get_value(torch.cat(states))
            new_log_probs = torch.cat([model.get_action(s)[1] for s in states])

            ratios = torch.exp(new_log_probs - torch.cat(log_probs).detach())
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - clip_ratio, 1 + clip_ratio) * advantages

            policy_loss = -torch.min(surr1, surr2).mean()
            value_loss = (new_values.squeeze() - returns).pow(2).mean()

            # Added entropy bonus for exploration
            new_entropy = torch.cat([model.get_action(s)[2] for s in states])
            entropy_bonus = new_entropy.mean()

            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()

        # Learning rate decay
        lr_scheduler.step()

        # Periodic reporting
        if (episode + 1) % 20 == 0:
            avg_reward = np.mean(episode_rewards[-20:])
            success_rate = np.mean(success_records[-20:]) * 100
            print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Success Rate: {success_rate:.2f}%")

    plot_success_rate(success_records)
    return model

def compute_gae(rewards, values, gamma, lam=0.95):
    returns = []
    advantages = []
    gae = 0
    for i in reversed(range(len(rewards))):
        delta = rewards[i] + gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]
        gae = delta + gamma * lam * gae
        advantages.insert(0, gae)
        returns.insert(0, rewards[i] + gamma * (returns[0] if returns else 0))

    returns = torch.tensor(returns)
    advantages = torch.tensor(advantages)

    # Normalize advantages with robust handling
    std = advantages.std()
    if std > 1e-8:
        advantages = (advantages - advantages.mean()) / std

    return returns, advantages

def plot_success_rate(success_records):
    success_rate = [np.mean(success_records[max(0, i-49):i+1]) * 100 for i in range(len(success_records))]
    plt.figure(figsize=(10, 5))
    plt.plot(success_rate, label="Success Rate (Moving Avg.)", color='b')
    plt.xlabel("Episodes")
    plt.ylabel("Success Rate (%)")
    plt.title("Success Rate vs Episodes")
    plt.legend()
    plt.grid()
    plt.show()

if __name__ == "__main__":
    lidar_resolution = 360
    env = StaticObstacleEnv(lidar_resolution=lidar_resolution)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    model = PPOModel(state_dim, action_dim)
    model = train_ppo(env, model)