# -*- coding: utf-8 -*-
"""ppo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xTCYM-Ux6ZKkim4VFzVE2mtU2c9Gg_Z
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from gym import spaces
import gym
from torch.cuda.amp import autocast, GradScaler
from typing import Tuple, Dict, List
import time

# Check for CUDA availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ================= Environment: StaticObstacleEnv =================
class StaticObstacleEnv(gym.Env):
    def __init__(self, lidar_resolution=180):  # Reduced lidar resolution
        super(StaticObstacleEnv, self).__init__()
        self.grid_size = 6  # Further reduced grid size
        self.num_obstacles = 2  # Further reduced obstacles
        self.goal_position = np.array([5, 5])  # Adjusted goal position
        self.lidar_resolution = lidar_resolution
        self.previous_dist = None
        self.min_obstacle_dist_to_goal = 1.5  # Minimum distance obstacles must be from goal

        # Precompute angles for lidar
        self.lidar_angles = np.linspace(0, 2 * np.pi, self.lidar_resolution, endpoint=False)

        # Action Space: [vx, vy]
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )

        # Observation Space
        self.observation_space = spaces.Box(
            low=0.0,
            high=self.grid_size,
            shape=(5 + 2 * self.num_obstacles + self.lidar_resolution,),
            dtype=np.float32
        )

    def reset(self):
        # Fixed start position for consistency
        self.agent_position = np.array([1.0, 1.0])
        self.agent_orientation = 0.0
        self.previous_dist = None

        # More strategic obstacle placement
        self.obstacle_positions = []
        while len(self.obstacle_positions) < self.num_obstacles:
            pos = np.random.uniform(1.5, self.grid_size-1.5, 2)
            # Check distance from start, goal, and other obstacles
            if (np.linalg.norm(pos - self.agent_position) > 1.5 and
                np.linalg.norm(pos - self.goal_position) > self.min_obstacle_dist_to_goal and
                all(np.linalg.norm(pos - existing_obs) > 1.5 for existing_obs in self.obstacle_positions)):
                self.obstacle_positions.append(pos)

        self.obstacle_positions = np.array(self.obstacle_positions)
        self.steps = 0
        self.max_steps = 100  # Reduced max steps
        return self._get_observation()

    def _get_observation(self):
        lidar_scan = self._simulate_lidar_scan()
        return np.concatenate([
            self.agent_position,
            np.array([self.agent_orientation]),
            self.goal_position,
            self.obstacle_positions.flatten(),
            lidar_scan
        ])

    def step(self, action):
        self.steps += 1

        # Smoother movement
        vx, vy = np.clip(action, self.action_space.low, self.action_space.high)
        old_position = self.agent_position.copy()
        self.agent_position += np.array([vx, vy]) * 0.5  # Reduced step size
        self.agent_position = np.clip(self.agent_position, 0, self.grid_size)

        # Calculate distances
        dist_to_goal = np.linalg.norm(self.agent_position - self.goal_position)
        min_obstacle_dist = min(np.linalg.norm(self.agent_position - obs) for obs in self.obstacle_positions)

        # Get reward and done flag
        reward, done = self._compute_reward_and_done(dist_to_goal, min_obstacle_dist, old_position)

        # Check for timeout
        if self.steps >= self.max_steps:
            done = True
            reward -= 20  # Reduced timeout penalty

        return self._get_observation(), reward, done, {'success': dist_to_goal < 0.5}

    def _compute_reward_and_done(self, dist_to_goal: float, min_obstacle_dist: float, old_position: np.ndarray) -> Tuple[float, bool]:
        # Success reward
        if dist_to_goal < 0.5:
            return 500.0, True  # Increased success reward significantly

        # Collision penalty
        if min_obstacle_dist < 0.5:
            return -200.0, True  # Increased collision penalty

        # Initialize reward components
        reward = 0.0

        # Progress reward
        if self.previous_dist is None:
            self.previous_dist = dist_to_goal
        progress = self.previous_dist - dist_to_goal
        progress_reward = 50.0 * progress  # Increased progress reward
        reward += progress_reward
        self.previous_dist = dist_to_goal

        # Distance-based reward
        distance_reward = -2.0 * dist_to_goal  # Linear penalty for distance
        reward += distance_reward

        # Obstacle avoidance reward
        if min_obstacle_dist < 1.0:
            obstacle_reward = 5.0 * (min_obstacle_dist - 0.5)  # Reward for keeping distance from obstacles
            reward += obstacle_reward

        # Movement efficiency reward
        movement = np.linalg.norm(self.agent_position - old_position)
        if progress <= 0:  # If not making progress
            reward -= 2.0 * movement  # Penalty for unnecessary movement

        # Boundary penalty
        if any(self.agent_position <= 0.2) or any(self.agent_position >= self.grid_size - 0.2):
            reward -= 2.0

        return reward, False

    def _simulate_lidar_scan(self):
        lidar_scan = np.full(self.lidar_resolution, self.grid_size)
        for i, angle in enumerate(self.lidar_angles):
            ray_direction = np.array([np.cos(angle), np.sin(angle)])
            for obstacle in self.obstacle_positions:
                to_obstacle = obstacle - self.agent_position
                proj = np.dot(to_obstacle, ray_direction)
                if 0 < proj < self.grid_size:
                    perp_dist = np.linalg.norm(to_obstacle - proj * ray_direction)
                    if perp_dist < 0.5:
                        lidar_scan[i] = min(lidar_scan[i], proj)
        return lidar_scan

# ================= Model: PPO =================
class PPOModel(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 1024):
        super(PPOModel, self).__init__()

        # Input processing
        self.layer_norm_input = nn.LayerNorm(state_dim)

        # Shared layers
        self.shared_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),

            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
        )

        # Actor head
        self.actor_net = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.GELU(),
        )

        self.actor_mean = nn.Linear(hidden_dim // 4, action_dim)
        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))

        # Critic head
        self.critic_net = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, 1)
        )

        self.apply(self._init_weights)
        self.to(device)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

    def forward(self, state):
        x = self.layer_norm_input(state)
        shared_features = self.shared_net(x)
        return shared_features

    def get_action(self, state):
        shared_features = self.forward(state)

        # Actor head
        actor_features = self.actor_net(shared_features)
        mean = torch.tanh(self.actor_mean(actor_features))
        std = torch.exp(self.actor_log_std.clamp(-20, 2))

        # Create distribution
        dist = torch.distributions.Normal(mean, std)
        action = dist.rsample()
        action = torch.clamp(action, -1.0, 1.0)
        log_prob = dist.log_prob(action).sum(dim=-1)
        entropy = dist.entropy().mean()

        return action, log_prob, entropy

    def get_value(self, state):
        shared_features = self.forward(state)
        return self.critic_net(shared_features)

def compute_gae(rewards: List[float], values: torch.Tensor, gamma: float = 0.99, lam: float = 0.95) -> Tuple[torch.Tensor, torch.Tensor]:
    values = values.detach().cpu().numpy()
    advantages = np.zeros_like(rewards, dtype=np.float32)
    returns = np.zeros_like(rewards, dtype=np.float32)

    gae = 0
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]

        delta = rewards[t] + gamma * next_value - values[t]
        gae = delta + gamma * lam * gae
        advantages[t] = gae
        returns[t] = advantages[t] + values[t]

    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)
    returns = torch.tensor(returns, dtype=torch.float32, device=device)

    # Normalize advantages
    if len(advantages) > 1:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    return returns, advantages

def train_ppo(env: gym.Env, model: PPOModel, num_episodes: int = 2000,
              gamma: float = 0.99, lr: float = 1e-4, clip_ratio: float = 0.2,
              batch_size: int = 64) -> PPOModel:

    optimizer = optim.AdamW(model.parameters(), lr=lr, eps=1e-5, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_episodes, eta_min=1e-5)
    scaler = GradScaler()  # For mixed precision training

    episode_rewards = []
    success_records = []
    best_success_rate = 0
    best_model_state = None

    start_time = time.time()

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0
        trajectory = {'states': [], 'actions': [], 'rewards': [], 'log_probs': [], 'values': []}

        # Collect trajectory
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

            with torch.no_grad():
                with autocast():  # Mixed precision inference
                    action, log_prob, _ = model.get_action(state_tensor)
                    value = model.get_value(state_tensor)

            next_state, reward, done, info = env.step(action.cpu().numpy()[0])

            trajectory['states'].append(state_tensor)
            trajectory['actions'].append(action)
            trajectory['rewards'].append(reward)
            trajectory['log_probs'].append(log_prob)
            trajectory['values'].append(value)

            total_reward += reward
            state = next_state

        success_records.append(1 if info.get('success', False) else 0)
        episode_rewards.append(total_reward)

        # Process trajectory data
        states = torch.cat(trajectory['states'])
        actions = torch.cat(trajectory['actions'])
        old_log_probs = torch.stack(trajectory['log_probs']).to(device)
        old_values = torch.cat(trajectory['values'])
        rewards = trajectory['rewards']

        # Compute returns and advantages
        returns, advantages = compute_gae(rewards, old_values, gamma)

        # PPO update with mini-batches
        dataset_size = len(states)
        num_updates = 8  # Increased number of updates per episode

        for _ in range(num_updates):
            # Generate random indices for mini-batches
            indices = np.random.permutation(dataset_size)

            for start_idx in range(0, dataset_size, batch_size):
                batch_indices = indices[start_idx:start_idx + batch_size]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # Mixed precision training
                with autocast():
                    # Get current policy outputs
                    _, new_log_probs, entropy = model.get_action(batch_states)
                    new_values = model.get_value(batch_states)

                    # Policy loss
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)
                    surr1 = ratio * batch_advantages
                    surr2 = torch.clamp(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * batch_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()

                    # Value loss
                    value_loss = 0.5 * (batch_returns - new_values.squeeze()).pow(2).mean()

                    # Entropy loss
                    entropy_loss = -0.01 * entropy

                    # Total loss
                    loss = policy_loss + value_loss + entropy_loss

                # Optimizer step with gradient scaling
                optimizer.zero_grad()
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                scaler.step(optimizer)
                scaler.update()

        scheduler.step()

        # Logging and model saving
        if (episode + 1) % 10 == 0:
            recent_success_rate = np.mean(success_records[-10:]) * 100
            avg_reward = np.mean(episode_rewards[-10:])
            elapsed_time = time.time() - start_time
            print(f"Episode {episode+1}/{num_episodes}, Success Rate: {recent_success_rate:.1f}%, "
                  f"Avg Reward: {avg_reward:.1f}, Time: {elapsed_time:.1f}s")

            if recent_success_rate > best_success_rate:
                best_success_rate = recent_success_rate
                best_model_state = model.state_dict().copy()
                print(f"New best success rate: {best_success_rate:.1f}%")

    # Restore best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nRestored best model with {best_success_rate:.1f}% success rate")

    # Plot final results
    plot_training_results(episode_rewards, success_records)

    return model

def plot_training_results(episode_rewards, success_records):
    plt.style.use('seaborn')
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))

    # Plot rewards
    rewards_mean = np.array([np.mean(episode_rewards[max(0, i-49):i+1])
                            for i in range(len(episode_rewards))])
    rewards_std = np.array([np.std(episode_rewards[max(0, i-49):i+1])
                           for i in range(len(episode_rewards))])

    episodes = np.arange(len(episode_rewards))
    ax1.plot(episodes, rewards_mean, label='Mean Reward (50 ep. window)', color='blue')
    ax1.fill_between(episodes, rewards_mean - rewards_std, rewards_mean + rewards_std,
                     alpha=0.2, color='blue')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('Training Rewards')
    ax1.legend()
    ax1.grid(True)

    # Plot success rate
    window = 50
    success_rate = [np.mean(success_records[max(0, i-window+1):i+1]) * 100
                   for i in range(len(success_records))]
    ax2.plot(success_rate, label=f'Success Rate ({window} ep. window)',
             color='green', linewidth=2)
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Success Rate (%)')
    ax2.set_title('Training Success Rate')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

def evaluate_model(env, model, num_episodes=100):
    """Evaluate the model over multiple episodes."""
    success_count = 0
    total_rewards = []
    step_counts = []

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        episode_reward = 0
        steps = 0

        while not done:
            with torch.no_grad():
                with autocast():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                    action, _, _ = model.get_action(state_tensor)

            next_state, reward, done, info = env.step(action.cpu().numpy()[0])
            episode_reward += reward
            state = next_state
            steps += 1

            if info.get('success', False):
                success_count += 1
                break

        total_rewards.append(episode_reward)
        step_counts.append(steps)

    success_rate = (success_count / num_episodes) * 100
    avg_reward = np.mean(total_rewards)
    avg_steps = np.mean(step_counts)

    print(f"\nEvaluation Results over {num_episodes} episodes:")
    print(f"Success Rate: {success_rate:.1f}%")
    print(f"Average Reward: {avg_reward:.1f}")
    print(f"Average Steps to Goal: {avg_steps:.1f}")

    return success_rate, avg_reward, avg_steps

if __name__ == "__main__":
    # Enable TensorFloat32 (TF32) for better performance on Ampere GPUs
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from gym import spaces
import gym
from torch.cuda.amp import autocast, GradScaler
from typing import Tuple, Dict, List
import time

# Check for CUDA availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ================= Environment: StaticObstacleEnv =================
class StaticObstacleEnv(gym.Env):
    def __init__(self, lidar_resolution=180):  # Reduced lidar resolution
        super(StaticObstacleEnv, self).__init__()
        self.grid_size = 6  # Further reduced grid size
        self.num_obstacles = 2  # Further reduced obstacles
        self.goal_position = np.array([5, 5])  # Adjusted goal position
        self.lidar_resolution = lidar_resolution
        self.previous_dist = None
        self.min_obstacle_dist_to_goal = 1.5  # Minimum distance obstacles must be from goal

        # Precompute angles for lidar
        self.lidar_angles = np.linspace(0, 2 * np.pi, self.lidar_resolution, endpoint=False)

        # Action Space: [vx, vy]
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )

        # Observation Space
        self.observation_space = spaces.Box(
            low=0.0,
            high=self.grid_size,
            shape=(5 + 2 * self.num_obstacles + self.lidar_resolution,),
            dtype=np.float32
        )

    def reset(self):
        # Fixed start position for consistency
        self.agent_position = np.array([1.0, 1.0])
        self.agent_orientation = 0.0
        self.previous_dist = None

        # More strategic obstacle placement
        self.obstacle_positions = []
        while len(self.obstacle_positions) < self.num_obstacles:
            pos = np.random.uniform(1.5, self.grid_size-1.5, 2)
            # Check distance from start, goal, and other obstacles
            if (np.linalg.norm(pos - self.agent_position) > 1.5 and
                np.linalg.norm(pos - self.goal_position) > self.min_obstacle_dist_to_goal and
                all(np.linalg.norm(pos - existing_obs) > 1.5 for existing_obs in self.obstacle_positions)):
                self.obstacle_positions.append(pos)

        self.obstacle_positions = np.array(self.obstacle_positions)
        self.steps = 0
        self.max_steps = 100  # Reduced max steps
        return self._get_observation()

    def _get_observation(self):
        lidar_scan = self._simulate_lidar_scan()
        return np.concatenate([
            self.agent_position,
            np.array([self.agent_orientation]),
            self.goal_position,
            self.obstacle_positions.flatten(),
            lidar_scan
        ])

    def step(self, action):
        self.steps += 1

        # Smoother movement
        vx, vy = np.clip(action, self.action_space.low, self.action_space.high)
        old_position = self.agent_position.copy()
        self.agent_position += np.array([vx, vy]) * 0.5  # Reduced step size
        self.agent_position = np.clip(self.agent_position, 0, self.grid_size)

        # Calculate distances
        dist_to_goal = np.linalg.norm(self.agent_position - self.goal_position)
        min_obstacle_dist = min(np.linalg.norm(self.agent_position - obs) for obs in self.obstacle_positions)

        # Get reward and done flag
        reward, done = self._compute_reward_and_done(dist_to_goal, min_obstacle_dist, old_position)

        # Check for timeout
        if self.steps >= self.max_steps:
            done = True
            reward -= 20  # Reduced timeout penalty

        return self._get_observation(), reward, done, {'success': dist_to_goal < 0.5}

    def _compute_reward_and_done(self, dist_to_goal: float, min_obstacle_dist: float, old_position: np.ndarray) -> Tuple[float, bool]:
        # Success reward
        if dist_to_goal < 0.5:
            return 500.0, True  # Increased success reward significantly

        # Collision penalty
        if min_obstacle_dist < 0.5:
            return -200.0, True  # Increased collision penalty

        # Initialize reward components
        reward = 0.0

        # Progress reward
        if self.previous_dist is None:
            self.previous_dist = dist_to_goal
        progress = self.previous_dist - dist_to_goal
        progress_reward = 50.0 * progress  # Increased progress reward
        reward += progress_reward
        self.previous_dist = dist_to_goal

        # Distance-based reward
        distance_reward = -2.0 * dist_to_goal  # Linear penalty for distance
        reward += distance_reward

        # Obstacle avoidance reward
        if min_obstacle_dist < 1.0:
            obstacle_reward = 5.0 * (min_obstacle_dist - 0.5)  # Reward for keeping distance from obstacles
            reward += obstacle_reward

        # Movement efficiency reward
        movement = np.linalg.norm(self.agent_position - old_position)
        if progress <= 0:  # If not making progress
            reward -= 2.0 * movement  # Penalty for unnecessary movement

        # Boundary penalty
        if any(self.agent_position <= 0.2) or any(self.agent_position >= self.grid_size - 0.2):
            reward -= 2.0

        return reward, False

    def _simulate_lidar_scan(self):
        lidar_scan = np.full(self.lidar_resolution, self.grid_size)
        for i, angle in enumerate(self.lidar_angles):
            ray_direction = np.array([np.cos(angle), np.sin(angle)])
            for obstacle in self.obstacle_positions:
                to_obstacle = obstacle - self.agent_position
                proj = np.dot(to_obstacle, ray_direction)
                if 0 < proj < self.grid_size:
                    perp_dist = np.linalg.norm(to_obstacle - proj * ray_direction)
                    if perp_dist < 0.5:
                        lidar_scan[i] = min(lidar_scan[i], proj)
        return lidar_scan

# ================= Model: PPO =================
class PPOModel(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 1024):
        super(PPOModel, self).__init__()

        # Input processing
        self.layer_norm_input = nn.LayerNorm(state_dim)

        # Shared layers
        self.shared_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),

            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
        )

        # Actor head
        self.actor_net = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.GELU(),
        )

        self.actor_mean = nn.Linear(hidden_dim // 4, action_dim)
        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))

        # Critic head
        self.critic_net = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, 1)
        )

        self.apply(self._init_weights)
        self.to(device)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

    def forward(self, state):
        x = self.layer_norm_input(state)
        shared_features = self.shared_net(x)
        return shared_features

    def get_action(self, state):
        shared_features = self.forward(state)

        # Actor head
        actor_features = self.actor_net(shared_features)
        mean = torch.tanh(self.actor_mean(actor_features))
        std = torch.exp(self.actor_log_std.clamp(-20, 2))

        # Create distribution
        dist = torch.distributions.Normal(mean, std)
        action = dist.rsample()
        action = torch.clamp(action, -1.0, 1.0)
        log_prob = dist.log_prob(action).sum(dim=-1)
        entropy = dist.entropy().mean()

        return action, log_prob, entropy

    def get_value(self, state):
        shared_features = self.forward(state)
        return self.critic_net(shared_features)

def compute_gae(rewards: List[float], values: torch.Tensor, gamma: float = 0.99, lam: float = 0.95) -> Tuple[torch.Tensor, torch.Tensor]:
    values = values.detach().cpu().numpy()
    advantages = np.zeros_like(rewards, dtype=np.float32)
    returns = np.zeros_like(rewards, dtype=np.float32)

    gae = 0
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]

        delta = rewards[t] + gamma * next_value - values[t]
        gae = delta + gamma * lam * gae
        advantages[t] = gae
        returns[t] = advantages[t] + values[t]

    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)
    returns = torch.tensor(returns, dtype=torch.float32, device=device)

    # Normalize advantages
    if len(advantages) > 1:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    return returns, advantages

def train_ppo(env: gym.Env, model: PPOModel, num_episodes: int = 2000,
              gamma: float = 0.99, lr: float = 1e-4, clip_ratio: float = 0.2,
              batch_size: int = 64) -> PPOModel:

    optimizer = optim.AdamW(model.parameters(), lr=lr, eps=1e-5, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_episodes, eta_min=1e-5)
    scaler = GradScaler()  # For mixed precision training

    episode_rewards = []
    success_records = []
    best_success_rate = 0
    best_model_state = None

    start_time = time.time()

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0
        trajectory = {'states': [], 'actions': [], 'rewards': [], 'log_probs': [], 'values': []}

        # Collect trajectory
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

            with torch.no_grad():
                with autocast():  # Mixed precision inference
                    action, log_prob, _ = model.get_action(state_tensor)
                    value = model.get_value(state_tensor)

            next_state, reward, done, info = env.step(action.cpu().numpy()[0])

            trajectory['states'].append(state_tensor)
            trajectory['actions'].append(action)
            trajectory['rewards'].append(reward)
            trajectory['log_probs'].append(log_prob)
            trajectory['values'].append(value)

            total_reward += reward
            state = next_state

        success_records.append(1 if info.get('success', False) else 0)
        episode_rewards.append(total_reward)

        # Process trajectory data
        states = torch.cat(trajectory['states'])
        actions = torch.cat(trajectory['actions'])
        old_log_probs = torch.stack(trajectory['log_probs']).to(device)
        old_values = torch.cat(trajectory['values'])
        rewards = trajectory['rewards']

        # Compute returns and advantages
        returns, advantages = compute_gae(rewards, old_values, gamma)

        # PPO update with mini-batches
        dataset_size = len(states)
        num_updates = 8  # Increased number of updates per episode

        for _ in range(num_updates):
            # Generate random indices for mini-batches
            indices = np.random.permutation(dataset_size)

            for start_idx in range(0, dataset_size, batch_size):
                batch_indices = indices[start_idx:start_idx + batch_size]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # Mixed precision training
                with autocast():
                    # Get current policy outputs
                    _, new_log_probs, entropy = model.get_action(batch_states)
                    new_values = model.get_value(batch_states)

                    # Policy loss
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)
                    surr1 = ratio * batch_advantages
                    surr2 = torch.clamp(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * batch_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()

                    # Value loss
                    value_loss = 0.5 * (batch_returns - new_values.squeeze()).pow(2).mean()

                    # Entropy loss
                    entropy_loss = -0.01 * entropy

                    # Total loss
                    loss = policy_loss + value_loss + entropy_loss

                # Optimizer step with gradient scaling
                optimizer.zero_grad()
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                scaler.step(optimizer)
                scaler.update()

        scheduler.step()

        # Logging and model saving
        if (episode + 1) % 10 == 0:
            recent_success_rate = np.mean(success_records[-10:]) * 100
            avg_reward = np.mean(episode_rewards[-10:])
            elapsed_time = time.time() - start_time
            print(f"Episode {episode+1}/{num_episodes}, Success Rate: {recent_success_rate:.1f}%, "
                  f"Avg Reward: {avg_reward:.1f}, Time: {elapsed_time:.1f}s")

            if recent_success_rate > best_success_rate:
                best_success_rate = recent_success_rate
                best_model_state = model.state_dict().copy()
                print(f"New best success rate: {best_success_rate:.1f}%")

    # Restore best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nRestored best model with {best_success_rate:.1f}% success rate")

    # Plot final results
    plot_training_results(episode_rewards, success_records)

    return model

def plot_training_results(episode_rewards, success_records):
    plt.style.use('seaborn')
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))

    # Plot rewards
    rewards_mean = np.array([np.mean(episode_rewards[max(0, i-49):i+1])
                            for i in range(len(episode_rewards))])
    rewards_std = np.array([np.std(episode_rewards[max(0, i-49):i+1])
                           for i in range(len(episode_rewards))])

    episodes = np.arange(len(episode_rewards))
    ax1.plot(episodes, rewards_mean, label='Mean Reward (50 ep. window)', color='blue')
    ax1.fill_between(episodes, rewards_mean - rewards_std, rewards_mean + rewards_std,
                     alpha=0.2, color='blue')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('Training Rewards')
    ax1.legend()
    ax1.grid(True)

    # Plot success rate
    window = 50
    success_rate = [np.mean(success_records[max(0, i-window+1):i+1]) * 100
                   for i in range(len(success_records))]
    ax2.plot(success_rate, label=f'Success Rate ({window} ep. window)',
             color='green', linewidth=2)
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Success Rate (%)')
    ax2.set_title('Training Success Rate')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

def evaluate_model(env, model, num_episodes=100):
    """Evaluate the model over multiple episodes."""
    success_count = 0
    total_rewards = []
    step_counts = []

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        episode_reward = 0
        steps = 0

        while not done:
            with torch.no_grad():
                with autocast():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                    action, _, _ = model.get_action(state_tensor)

            next_state, reward, done, info = env.step(action.cpu().numpy()[0])
            episode_reward += reward
            state = next_state
            steps += 1

            if info.get('success', False):
                success_count += 1
                break

        total_rewards.append(episode_reward)
        step_counts.append(steps)

    success_rate = (success_count / num_episodes) * 100
    avg_reward = np.mean(total_rewards)
    avg_steps = np.mean(step_counts)

    print(f"\nEvaluation Results over {num_episodes} episodes:")
    print(f"Success Rate: {success_rate:.1f}%")
    print(f"Average Reward: {avg_reward:.1f}")
    print(f"Average Steps to Goal: {avg_steps:.1f}")

    return success_rate, avg_reward, avg_steps

if __name__ == "__main__":
    # Enable TensorFloat32 (TF32) for better performance on Ampere GPUs
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # Set random seeds for reproducibility
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    np.random.seed(42)

    # Create environment and model
    lidar_resolution = 180  # Reduced for efficiency
    env = StaticObstacleEnv(lidar_resolution=lidar_resolution)

    # Get dimensions
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    # Create model with improved architecture
    model = PPOModel(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=1024  # Increased network capacity
    )

    # Training parameters
    params = {
        'num_episodes': 2000,  # Increased episodes
        'gamma': 0.99,
        'lr': 1e-4,
        'clip_ratio': 0.2,
        'batch_size': 64
    }

    # Train the model
    print("Starting training...")
    print(f"Training parameters: {params}")

    trained_model = train_ppo(env, model, **params)

    # Evaluate the trained model
    print("\nEvaluating trained model...")
    evaluate_model(env, trained_model)
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    np.random.seed(42)

    # Create environment and model
    lidar_resolution = 180  # Reduced for efficiency
    env = StaticObstacleEnv(lidar_resolution=lidar_resolution)

    # Get dimensions
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    # Create model with improved architecture
    model = PPOModel(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=1024  # Increased network capacity
    )

    # Training parameters
    params = {
        'num_episodes': 2000,  # Increased episodes
        'gamma': 0.99,
        'lr': 1e-4,
        'clip_ratio': 0.2,
        'batch_size': 64
    }

    # Train the model
    print("Starting training...")
    print(f"Training parameters: {params}")

    trained_model = train_ppo(env, model, **params)

    # Evaluate the trained model
    print("\nEvaluating trained model...")
    evaluate_model(env, trained_model)